{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a47098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve Pubmed article details by either scraping a website or using a keyword or phrase\n",
    "#  see https://biopython-tutorial.readthedocs.io/en/latest/notebooks/09%20-%20Accessing%20NCBIs%20Entrez%20databases.html\n",
    "# (c) 2022-2023 RENCI, Chapel Hill, NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib3\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "Entrez.api_key = '7e2310a65401cdf4d5023cda2467c19de708'\n",
    "Entrez.email = 'hubal@email.unc.edu'\n",
    "Entrez.sleep_between_tries = 2 # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad676927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_articles_from_keyword(keyword, num_articles):\n",
    "    handle = Entrez.esearch(db='pubmed', term=keyword)\n",
    "    l = collect_article_details(Entrez.read(handle)['IdList'])\n",
    "    if len(l) > num_articles:\n",
    "        return l[0: num_articles]\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "def collect_articles_from_website(text):\n",
    "    return collect_article_details(retrieve_pmids(text))\n",
    "\n",
    "def collect_article_details(pmids):\n",
    "    df = create_df()\n",
    "    for pmid in pmids:\n",
    "        a = retrieve_xml_from_pmid(pmid)\n",
    "        df = append_to_df(df, [\n",
    "            pmid, extract_pmcid(a), extract_title(a),\n",
    "            extract_authors(a), extract_year(a), extract_journal(a),\n",
    "            extract_keywords(a),\n",
    "            extract_abstract(a), extract_grant(a), has_supplemental_data(extract_pmcid(a))])\n",
    "    return df\n",
    "\n",
    "def retrieve_xml_from_url(url):\n",
    "    q = requests.get(url)\n",
    "    return BeautifulSoup(q.content, 'html.parser').get_text()\n",
    "\n",
    "def retrieve_xml_from_pmid(pmid):\n",
    "    return Entrez.read(Entrez.efetch(db='pubmed', id=pmid, retmode='xml'))\n",
    "\n",
    "def retrieve_pmids(text):\n",
    "    i = 1\n",
    "    r = []\n",
    "    while i > 0:\n",
    "        i = text.find('PMID', i+1)\n",
    "        t = text[i+6: i+14]\n",
    "        try:\n",
    "            r.append(int(t))\n",
    "        except ValueError:\n",
    "            r.append(-1)\n",
    "    return r\n",
    "\n",
    "def create_df():\n",
    "    return pd.DataFrame({\n",
    "        'PMID':[], 'PMCID':[], 'Title':[], 'Author':[], 'Year':[], 'Journal':[], 'Keywords':[], 'Abstract':[],\n",
    "        'Grant':[], 'Supplemental Data (Y/N)': []})\n",
    "\n",
    "def append_to_df(df, row):\n",
    "    df.loc[len(df.index)] = row\n",
    "    return df\n",
    "\n",
    "def extract_abstract(xml):\n",
    "    try:\n",
    "        abstract = xml['PubmedArticle'][0]['MedlineCitation']['Article']['Abstract']\n",
    "        return abstract\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def extract_authors(xml):\n",
    "    authors = []\n",
    "    try:\n",
    "        for i in range (0, len(xml['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList'])):\n",
    "            lname = xml['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList'][i]['LastName']\n",
    "            inits = xml['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList'][i]['Initials']\n",
    "            authors.append(lname + inits)\n",
    "        return authors\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def extract_grant(xml):\n",
    "    grants = []\n",
    "    try:\n",
    "        for i in range (0, len(xml['PubmedArticle'][0]['MedlineCitation']['Article']['GrantList'])):\n",
    "            grant = xml['PubmedArticle'][0]['MedlineCitation']['Article']['GrantList'][i]['GrantID']\n",
    "            agency = xml['PubmedArticle'][0]['MedlineCitation']['Article']['GrantList'][i]['Agency']\n",
    "            grants.append(agency + ', ' + grant)\n",
    "        return grants\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def extract_journal(xml):\n",
    "    try:\n",
    "        journal = xml['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']['Title']\n",
    "        return journal\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def extract_keywords(xml):\n",
    "    try:\n",
    "        keywords = xml['PubmedArticle'][0]['MedlineCitation']['KeywordList'][0][0]\n",
    "        return keywords\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def extract_pmcid(xml):\n",
    "    try:\n",
    "        pmcid = xml['PubmedArticle'][0]['PubmedData']['ArticleIdList']\n",
    "        for j in pmcid:\n",
    "            if j.startswith('PMC'):\n",
    "                return j[0: len(j)]\n",
    "        return None\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def extract_title(xml):\n",
    "    try:\n",
    "        title = xml['PubmedArticle'][0]['MedlineCitation']['Article']['ArticleTitle']\n",
    "        return title\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def extract_year(xml):\n",
    "    try:\n",
    "        year = xml['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "        return year\n",
    "    except IndexError:\n",
    "        return None\n",
    "    except KeyError:\n",
    "        return '0000'\n",
    "\n",
    "def form_url_from_pmcid(pmcid):\n",
    "    return 'http://eutils.ncbi.nlm.nih.gov/pmc/?term=(' + str(pmcid) + ')'\n",
    "\n",
    "def has_supplemental_data(pmcid):\n",
    "    t = retrieve_xml_from_url(form_url_from_pmcid(pmcid) + '+AND+has+suppdata%5Bfilter%5D')\n",
    "    if 'No items found' in t:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f330bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: retrieve article information from scraping a website\n",
    "data = collect_articles_from_website(retrieve_xml_from_url(\n",
    "    'https://heal.nih.gov/research/publications#translation-of-research-to-practice-for-the-treatment-of-opioid-addiction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be546685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: extract article information related to a given string\n",
    "data = collect_articles_from_keyword('Hepatitus B Virus', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb5464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
